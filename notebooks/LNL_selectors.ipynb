{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uy43buLIvVyv"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "from tqdm import tqdm\n",
        "from src.runner import run_baselines\n",
        "from src.utils import process_results\n",
        "from src.data_loader import *\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from random import sample\n",
        "from copy import deepcopy\n",
        "\n",
        "from src.dips_selector import *\n",
        "from src.data_loader import *\n",
        "from src.baseline_functions import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import cluster\n",
        "import math\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "\n",
        "from datagnosis.plugins.core.datahandler import DataHandler\n",
        "from datagnosis.plugins.core.models.simple_mlp import SimpleMLP\n",
        "from datagnosis.plugins import Plugins\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def fit_mixture(scores, labels, p_threshold=0.5):\n",
        "    '''\n",
        "    Assume the distribution of scores: bimodal gaussian mixture model\n",
        "    \n",
        "    return clean labels\n",
        "    that belongs to the clean cluster by fitting the score distribution to GMM\n",
        "    '''\n",
        "    \n",
        "    clean_labels = []\n",
        "    indexes = np.array(range(len(scores)))\n",
        "    for cls in np.unique(labels):\n",
        "        cls_index = indexes[labels==cls]\n",
        "        feats = scores[labels==cls]\n",
        "        feats_ = np.ravel(feats).astype(np.float).reshape(-1, 1)\n",
        "        gmm = GMM(n_components=2, covariance_type='full', tol=1e-6, max_iter=100)\n",
        "        \n",
        "        gmm.fit(feats_)\n",
        "        prob = gmm.predict_proba(feats_)\n",
        "        prob = prob[:,gmm.means_.argmax()]\n",
        "        clean_labels = prob > p_threshold \n",
        "        clean_labels = np.where(clean_labels==1)[0]\n",
        "    \n",
        "    return clean_labels\n",
        "\n",
        "\n",
        "def get_mean_vector(features, labels):\n",
        "    mean_vector_dict = {}\n",
        "    with tqdm(total=len(np.unique(labels))) as pbar:\n",
        "        for index in np.unique(labels):\n",
        "            v = np.mean(features[labels==index], axis=0)\n",
        "            mean_vector_dict[index] = v\n",
        "            pbar.update(1)\n",
        "            \n",
        "    return mean_vector_dict\n",
        "            \n",
        "def get_singular_vector(features, labels):\n",
        "    '''\n",
        "    To get top1 sigular vector in class-wise manner by using SVD of hidden feature vectors\n",
        "    features: hidden feature vectors of data (numpy)\n",
        "    labels: correspoding label list\n",
        "    '''\n",
        "    \n",
        "    singular_vector_dict = {}\n",
        "    with tqdm(total=len(np.unique(labels))) as pbar:\n",
        "        for index in np.unique(labels):\n",
        "            _, _, v = np.linalg.svd(features[labels==index])\n",
        "            singular_vector_dict[index] = v[0]\n",
        "            pbar.update(1)\n",
        "\n",
        "    return singular_vector_dict\n",
        "\n",
        "\n",
        "def get_score(singular_vector_dict, features, labels, normalization=True):\n",
        "    '''\n",
        "    Calculate the score providing the degree of showing whether the data is clean or not.\n",
        "    '''\n",
        "    if normalization:\n",
        "        scores = [np.abs(np.inner(singular_vector_dict[labels[indx]], feat/np.linalg.norm(feat))) for indx, feat in enumerate(tqdm(features))]\n",
        "    else:\n",
        "        scores = [np.abs(np.inner(singular_vector_dict[labels[indx]], feat)) for indx, feat in enumerate(tqdm(features))]\n",
        "        \n",
        "    return np.array(scores)\n",
        "\n",
        "def extract_topk(scores, labels, k):\n",
        "    '''\n",
        "    k: ratio to extract topk scores in class-wise manner\n",
        "    To obtain the most prominsing clean data in each classes\n",
        "    \n",
        "    return selected labels \n",
        "    which contains top k data\n",
        "    '''\n",
        "    \n",
        "    indexes = torch.tensor(range(len(labels)))\n",
        "    selected_labels = []\n",
        "    for cls in np.unique(labels):\n",
        "        num = int(p * np.sum(labels==cls))\n",
        "        _, sorted_idx = torch.sort(scores[labels==cls], descending=True)\n",
        "        selected_labels += indexes[labels==cls][sorted_idx[:num]].numpy().tolist()\n",
        "        \n",
        "    return torch.tensor(selected_labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "def fine(current_features, current_labels, fit='kmeans', prev_features=None, prev_labels=None, p_threshold=0.5, norm=True, eigen=True):\n",
        "    '''\n",
        "    prev_features, prev_labels: data from the previous round\n",
        "    current_features, current_labels: current round's data\n",
        "    \n",
        "    return clean labels\n",
        "    \n",
        "    if you insert the prev_features and prev_labels to None,\n",
        "    the algorthm divides the data based on the current labels and current features\n",
        "    \n",
        "    '''\n",
        "    if eigen is True:\n",
        "        if prev_features is not None and prev_labels is not None:\n",
        "            vector_dict = get_singular_vector(prev_features, prev_labels)\n",
        "        else:\n",
        "            vector_dict = get_singular_vector(current_features, current_labels)\n",
        "    else:\n",
        "        if prev_features is not None and prev_labels is not None:\n",
        "            vector_dict = get_mean_vector(prev_features, prev_labels)\n",
        "        else:\n",
        "            vector_dict = get_mean_vector(current_features, current_labels)\n",
        "\n",
        "    scores = get_score(vector_dict, features = current_features, labels = current_labels, normalization=norm)\n",
        "    \n",
        "\n",
        "    clean_labels = fit_mixture(scores, current_labels, p_threshold=p_threshold)\n",
        "\n",
        "    return clean_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class UCI_MLP(nn.Module):\n",
        "    def __init__(self, num_features, num_outputs, dropout=0, batch_norm=False):\n",
        "        super(UCI_MLP, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.batch_norm = batch_norm\n",
        "        d = num_features + 1\n",
        "        self.fc1 = nn.Linear(num_features, d)\n",
        "        self.bn1 = nn.BatchNorm1d(d)\n",
        "        self.relu1 = nn.ReLU(inplace=False)\n",
        "        self.fc2 = nn.Linear(d, d)\n",
        "        self.bn2 = nn.BatchNorm1d(d)\n",
        "        self.relu2 = nn.ReLU(inplace=False)\n",
        "        self.fc3 = nn.Linear(d, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        out = self.fc1(x)\n",
        "        if self.batch_norm and batch_size > 1:\n",
        "          out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        if self.batch_norm and batch_size > 1:\n",
        "          out = self.bn2(out)\n",
        "        h_output = self.relu2(out)\n",
        "        h_output = self.dropout(h_output)\n",
        "        out = self.fc3(h_output)\n",
        "        return out\n",
        "\n",
        "\n",
        "def selector(X, y, method='loss', epochs=100, n_classes=2):\n",
        "\n",
        "            y = y.reshape(-1)\n",
        "            n_classes = len(np.unique(y))\n",
        "            datahander = DataHandler(X, y, batch_size=len(y))\n",
        "\n",
        "            # creating our model object, which we both want to use downstream, but also we will use to judge the hardness of the data points\n",
        "            model = UCI_MLP(num_features=X.shape[1], num_outputs=n_classes)\n",
        "            \n",
        "            #model = SimpleMLP(input_dim = X.shape[1], output_dim=2)\n",
        "\n",
        "            # creating our optimizer and loss function objects\n",
        "            learning_rate = 0.01\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "            if method=='dips':\n",
        "                y = y.reshape(-1)\n",
        "                datahander = DataHandler(X, y, batch_size=32)\n",
        "\n",
        "                hcm = Plugins().get(\n",
        "                    \"dips\",\n",
        "                    model=model,\n",
        "                    criterion=criterion,\n",
        "                    optimizer=optimizer,\n",
        "                    lr=learning_rate,\n",
        "                    epochs=epochs,\n",
        "                    num_classes=n_classes,\n",
        "                    logging_interval=1,\n",
        "                )\n",
        "\n",
        "                #Now plot the distribution of dips_xmetric\n",
        "                dips_xthresh = 0.15\n",
        "            elif method=='loss':\n",
        "                hcm = Plugins().get(\n",
        "                    \"large_loss\",\n",
        "                    model=model,\n",
        "                    criterion=criterion,\n",
        "                    optimizer=optimizer,\n",
        "                    lr=learning_rate,\n",
        "                    epochs=epochs,\n",
        "                    num_classes=n_classes,\n",
        "                    logging_interval=1,\n",
        "                )\n",
        "                print('LOSS...')\n",
        "\n",
        "            \n",
        "\n",
        "            elif method=='filter':\n",
        "                hcm = Plugins().get(\n",
        "                    \"filtering\",\n",
        "                    model=model,\n",
        "                    criterion=criterion,\n",
        "                    optimizer=optimizer,\n",
        "                    lr=learning_rate,\n",
        "                    epochs=epochs,\n",
        "                    num_classes=n_classes,\n",
        "                    logging_interval=1,\n",
        "                    total_samples = len(y)\n",
        "                )\n",
        "                print('Forgetting...')\n",
        "\n",
        "            elif method=='basicfilter':\n",
        "                hcm = Plugins().get(\n",
        "                    \"basicfilter\",\n",
        "                    model=model,\n",
        "                    criterion=criterion,\n",
        "                    optimizer=optimizer,\n",
        "                    lr=learning_rate,\n",
        "                    epochs=epochs,\n",
        "                    num_classes=n_classes,\n",
        "                    logging_interval=1,\n",
        "                    total_samples = len(y)\n",
        "                )\n",
        "                print('Forgetting...')\n",
        "\n",
        "            if method!='fine':\n",
        "                hcm.fit(\n",
        "                    datahandler=datahander,\n",
        "                    use_caches_if_exist=False,\n",
        "                )\n",
        "\n",
        "    \n",
        "            if method=='dips':\n",
        "                confidence, dips_xmetric = hcm.scores\n",
        "                dips_ythresh=0.2\n",
        "                easy_train, ambig_train, hard_train = get_groups(\n",
        "                    confidence=confidence,\n",
        "                    aleatoric_uncertainty=dips_xmetric,\n",
        "                    dips_xthresh=dips_xthresh,\n",
        "                    dips_ythresh=dips_ythresh,\n",
        "                )\n",
        "            elif method=='loss':\n",
        "                scores = hcm.scores\n",
        "                threshold = np.percentile(scores,99)\n",
        "                easy_train = np.where(scores<threshold)[0]\n",
        "            elif method=='filter' or method=='basicfilter':\n",
        "                scores = hcm.scores\n",
        "                easy_train = np.where(scores==1)[0]\n",
        "\n",
        "            if method == 'fine':\n",
        "                try:\n",
        "                    easy_train = fine(current_features=X, current_labels=y)\n",
        "                except:\n",
        "                    easy_train = np.arange(len(y))\n",
        "\n",
        "            if len(np.unique(y[easy_train])) != len(np.unique(y)):\n",
        "                # find one id of each unique label and append to easy_train\n",
        "                for label in np.unique(y):\n",
        "                    easy_train = np.append(easy_train, np.where(y==label)[0][0])\n",
        "                \n",
        "                # remove duplicates in easy_train\n",
        "                easy_train = np.unique(easy_train)\n",
        "\n",
        "            if len(easy_train)<2*len(np.unique(y)):\n",
        "                easy_train = np.arange(len(y))\n",
        "\n",
        "            return easy_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running dataset seer with filter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "Forgetting...\n",
            "(1600,) 1600\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[793 803]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[793 803]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "(1596,) 1596\n",
            "iteration  1\n",
            "(6397,) 6397\n",
            "iteration  2\n",
            "(8957,) 8957\n",
            "iteration  3\n",
            "(10366,) 10366\n",
            "iteration  4\n",
            "(11117,) 11117\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "(1600,) 1600\n",
            "iteration  1\n",
            "(6400,) 6400\n",
            "iteration  2\n",
            "(8961,) 8961\n",
            "iteration  3\n",
            "(10370,) 10370\n",
            "iteration  4\n",
            "(11122,) 11122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [01:56<03:53, 116.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "Forgetting...\n",
            "(1600,) 1600\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[784 810]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[784 810]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "(1594,) 1594\n",
            "iteration  1\n",
            "(6395,) 6395\n",
            "iteration  2\n",
            "(8955,) 8955\n",
            "iteration  3\n",
            "(10364,) 10364\n",
            "iteration  4\n",
            "(11116,) 11116\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "(1600,) 1600\n",
            "iteration  1\n",
            "(6400,) 6400\n",
            "iteration  2\n",
            "(8961,) 8961\n",
            "iteration  3\n",
            "(10370,) 10370\n",
            "iteration  4\n",
            "(11121,) 11121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [04:12<02:07, 127.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "Forgetting...\n",
            "(1600,) 1600\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[768 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[768 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "(1599,) 1599\n",
            "iteration  1\n",
            "(6400,) 6400\n",
            "iteration  2\n",
            "(8961,) 8961\n",
            "iteration  3\n",
            "(10370,) 10370\n",
            "iteration  4\n",
            "(11122,) 11122\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "(1600,) 1600\n",
            "iteration  1\n",
            "(6400,) 6400\n",
            "iteration  2\n",
            "(8961,) 8961\n",
            "iteration  3\n",
            "(10370,) 10370\n",
            "iteration  4\n",
            "(11121,) 11121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [06:27<00:00, 129.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running dataset seer with basicfilter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "Forgetting...\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[745 553]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[745 553]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [02:24<04:49, 144.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "Forgetting...\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[738 560]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[738 560]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [04:41<02:19, 139.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "Forgetting...\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[698 434]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[698 434]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [06:52<00:00, 137.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running dataset seer with loss\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "LOSS...\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[780 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[780 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [03:01<06:03, 181.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "LOSS...\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[772 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[772 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [06:18<03:10, 190.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n",
            "LOSS...\n",
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[753 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[753 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [10:14<00:00, 204.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running dataset seer with fine\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 48.73it/s]\n",
            "100%|██████████| 1600/1600 [00:00<00:00, 40653.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[225 218]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[225 218]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 181.72it/s]\n",
            "100%|██████████| 443/443 [00:00<00:00, 59712.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  5.91it/s]\n",
            "100%|██████████| 5244/5244 [00:00<00:00, 53546.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.20it/s]\n",
            "100%|██████████| 7805/7805 [00:00<00:00, 47048.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
            "100%|██████████| 9213/9213 [00:00<00:00, 41739.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
            "100%|██████████| 9772/9772 [00:00<00:00, 33438.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Pseudo_Labeling\n",
            "[796 804]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 35.94it/s]\n",
            "100%|██████████| 1600/1600 [00:00<00:00, 32965.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\n",
            "100%|██████████| 6400/6400 [00:00<00:00, 29494.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.38s/it]\n",
            "100%|██████████| 8961/8961 [00:00<00:00, 22291.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n",
            "100%|██████████| 10370/10370 [00:00<00:00, 31631.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.39s/it]\n",
            "100%|██████████| 11122/11122 [00:00<00:00, 35566.60it/s]\n",
            " 33%|███▎      | 1/3 [01:15<02:31, 75.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 2/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 42.90it/s]\n",
            "100%|██████████| 1600/1600 [00:00<00:00, 36204.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[194 210]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[194 210]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 217.58it/s]\n",
            "100%|██████████| 404/404 [00:00<00:00, 22467.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.85it/s]\n",
            "100%|██████████| 5205/5205 [00:00<00:00, 43588.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n",
            "100%|██████████| 7766/7766 [00:00<00:00, 38070.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n",
            "100%|██████████| 9174/9174 [00:00<00:00, 38750.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n",
            "100%|██████████| 9926/9926 [00:00<00:00, 33960.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Pseudo_Labeling\n",
            "[788 812]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 39.96it/s]\n",
            "100%|██████████| 1600/1600 [00:00<00:00, 37065.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n",
            "100%|██████████| 6400/6400 [00:00<00:00, 38524.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n",
            "100%|██████████| 8961/8961 [00:00<00:00, 52637.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
            "100%|██████████| 10370/10370 [00:00<00:00, 52306.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\n",
            "100%|██████████| 11121/11121 [00:00<00:00, 46713.15it/s]\n",
            " 67%|██████▋   | 2/3 [02:30<01:15, 75.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3/3\n",
            "# total samples = 20000 (1.0 - prop)\n",
            "# training points = 1600\n",
            "# test points = 4000\n",
            "# unlabelled points = 14400\n",
            "Training Fully Supervised model...\n",
            "Training Supervised model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 42.62it/s]\n",
            "100%|██████████| 1600/1600 [00:00<00:00, 27641.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Preprocess + Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[195 209]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[195 209]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 260.99it/s]\n",
            "100%|██████████| 404/404 [00:00<00:00, 42321.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n",
            "100%|██████████| 5205/5205 [00:00<00:00, 53323.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\n",
            "100%|██████████| 7766/7766 [00:00<00:00, 61478.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n",
            "100%|██████████| 9175/9175 [00:00<00:00, 47427.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n",
            "100%|██████████| 9927/9927 [00:00<00:00, 40385.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Pseudo_Labeling\n",
            "[769 831]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 32.56it/s]\n",
            "100%|██████████| 1600/1600 [00:00<00:00, 33578.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  2.46it/s]\n",
            "100%|██████████| 6400/6400 [00:00<00:00, 42077.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
            "100%|██████████| 8961/8961 [00:00<00:00, 31196.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n",
            "100%|██████████| 10370/10370 [00:00<00:00, 20841.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n",
            "100%|██████████| 11121/11121 [00:00<00:00, 38202.74it/s]\n",
            "100%|██████████| 3/3 [03:49<00:00, 76.55s/it]\n"
          ]
        }
      ],
      "source": [
        "import traceback\n",
        "\n",
        "methods = ['filter','basicfilter','loss','fine']\n",
        "\n",
        "#dataset_tuples = [('seer', '1.0'), ('adult', '0.66'), ('cutract', '1.0'), ('covid', '0.66'),   ('maggic', '0.66'), (\"compas\", \"1.0\"), (\"agaricus-lepiota\", '1.0'), ('German-credit', '1.0'), (\"higgs\", \"0.1\"),   ('drug', '0.1'), (\"blog\", \"0.2\"),(\"credit\", \"1.0\")]\n",
        "\n",
        "dataset_tuples = [('seer', '1.0'), ('adult', '0.66'), ('cutract', '1.0'), ('covid', '0.66'),   ('maggic', '0.66'), (\"compas\", \"1.0\"), (\"agaricus-lepiota\", '1.0'), ('German-credit', '1.0'), (\"higgs\", \"0.1\"),   ('drug', '0.1'), (\"blog\", \"0.2\"),(\"credit\", \"1.0\")]\n",
        "dataset_tuples = [('seer', '1.0')]\n",
        "\n",
        "from src.utils import (\n",
        "    append_acc_early_termination,\n",
        "    get_train_test_unlabeled,)\n",
        "\n",
        "for dataset_tuple in dataset_tuples:\n",
        "    results_store = {}\n",
        "    dataset = dataset_tuple[0]\n",
        "    prop_data= float(dataset_tuple[1])\n",
        "    for method in methods:\n",
        "        \n",
        "        try:\n",
        "\n",
        "            print(f\"running dataset {dataset} with {method}\")\n",
        "            overall_result_dicts = []\n",
        "            overall_data_dicts = []\n",
        "            overall_model_dicts = []\n",
        "\n",
        "            \n",
        "            dips_metric = 'aleatoric'\n",
        "            dips_ythresh = 0.2\n",
        "\n",
        "            algorithm_list=['Fully Supervised', 'Supervised_Learning','Pseudo_Labeling']\n",
        "                    \n",
        "            seed=0\n",
        "            nest=100\n",
        "\n",
        "            num_XGB_models=5\n",
        "            numTrials=3\n",
        "            numIters=5\n",
        "            upper_threshold=0.8\n",
        "            verbose=False\n",
        "            loss=True\n",
        "            epochs = 20\n",
        "\n",
        "\n",
        "            for i in tqdm(range(numTrials)):\n",
        "                try:\n",
        "                    split_prop = 1\n",
        "\n",
        "                    if dataset in [\"seer\", \"cutract\", \"covid\", \"support\", \"adult\", \"bank\", \"drug\",\"credit\", \"metabric\", \"fraud\", \"maggic\", 'bank', 'cover', 'higgs', 'contraceptive', 'blog', \"telescope\", \"bio\", \"eye\", \"compas\", \"marketing\",]:\n",
        "                        df_feat, df_label, df = get_data(dataset=dataset, prop=prop_data)\n",
        "\n",
        "                        x_train, x_test, y_train, y_test = train_test_split(\n",
        "                        df_feat, df_label, test_size=0.2, random_state=seed\n",
        "                        )\n",
        "\n",
        "                        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(\n",
        "                            x_train, y_train, train_size=0.1, random_state=seed\n",
        "                        )\n",
        "         \n",
        "                    else:\n",
        "                        prop_lab = 0.1\n",
        "                        path_to_file = \"./data/all_data.pickle\"\n",
        "                        (\n",
        "                            x_train,\n",
        "                            y_train,\n",
        "                            x_test,\n",
        "                            y_test,\n",
        "                            x_unlabeled,\n",
        "                            y_unlabeled,\n",
        "                        ) = get_train_test_unlabeled(dataset, prop_lab, path_to_file, random_state=seed)\n",
        "\n",
        "                    \n",
        "                    seed+=1\n",
        "\n",
        "                    print(f\"Trial {i+1}/{numTrials}\")\n",
        "                    results = {}\n",
        "                    data = {}\n",
        "                    models = {}\n",
        "\n",
        "\n",
        "                    x_unlabeled, x_test, y_test, x_train, y_train = (\n",
        "                        np.asarray(x_unlabeled),\n",
        "                        np.asarray(x_test),\n",
        "                        np.asarray(y_test),\n",
        "                        np.asarray(x_train),\n",
        "                        np.asarray(y_train),\n",
        "                    )\n",
        "                        \n",
        "\n",
        "                    datasize = x_train.shape\n",
        "\n",
        "                    total_samples = len(x_train) + len(x_test) + len(x_unlabeled)\n",
        "\n",
        "                    print(f\"# total samples = {total_samples} ({prop_data} - prop)\")\n",
        "\n",
        "                    print(f\"# training points = {y_train.shape[0]}\")\n",
        "\n",
        "                    print(f\"# test points = {y_test.shape[0]}\")\n",
        "\n",
        "                    print(f\"# unlabelled points = {x_unlabeled.shape[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "                    # # Supervised learning - Train an XGBoost model\n",
        "                    param = {}\n",
        "                    param[\"booster\"] = \"gbtree\"\n",
        "                    param[\"objective\"] = \"binary:logistic\"\n",
        "                    param[\"verbosity\"] = 0\n",
        "                    param[\"n_estimators\"] = nest\n",
        "                    param[\"silent\"] = 1\n",
        "                    param[\"seed\"] = seed\n",
        "\n",
        "                    print(\"Training Fully Supervised model...\")\n",
        "                    # create XGBoost instance with default hyper-parameters\n",
        "                    xgb = XGBClassifier(**param)\n",
        "                    all_x = np.concatenate((x_train, x_unlabeled))\n",
        "                    all_y = np.concatenate((y_train, y_unlabeled))\n",
        "                    \n",
        "                    xgb.fit(all_x, all_y)\n",
        "\n",
        "                    # evaluate the performance on the test set\n",
        "                    y_test_pred = xgb.predict(x_test)\n",
        "                    fully_supervised_learning_accuracy = np.round(\n",
        "                        accuracy_score(y_test_pred, y_test) * 100, 2\n",
        "                    )  # round to 2 digits xx.yy %\n",
        "\n",
        "                    results[\"fully_supervised_learning_accuracy\"] = fully_supervised_learning_accuracy\n",
        "\n",
        "\n",
        "                    print(\"Training Supervised model...\")\n",
        "                    # create XGBoost instance with default hyper-parameters\n",
        "                    xgb = XGBClassifier(**param)\n",
        "\n",
        "                    xgb.fit(x_train, y_train)\n",
        "\n",
        "                    # evaluate the performance on the test set\n",
        "                    y_test_pred = xgb.predict(x_test)\n",
        "                    supervised_learning_accuracy = np.round(\n",
        "                        accuracy_score(y_test_pred, y_test) * 100, 2\n",
        "                    )  # round to 2 digits xx.yy %\n",
        "\n",
        "                    results[\"supervised_learning_accuracy\"] = supervised_learning_accuracy\n",
        "\n",
        "                    # RUN DCAI\n",
        "                \n",
        "                    easy_train = selector(X=x_train, y=y_train, method=method, epochs=epochs)\n",
        "\n",
        "\n",
        "                    print(\"Training Preprocess + Supervised model...\")\n",
        "                    # create XGBoost instance with default hyper-parameters\n",
        "                    xgb = XGBClassifier(**param)\n",
        "\n",
        "                    xgb.fit(x_train[easy_train], y_train[easy_train])\n",
        "\n",
        "                    # evaluate the performance on the test set\n",
        "                    y_test_pred = xgb.predict(x_test)\n",
        "                    supervised_learning_accuracy_easy = np.round(\n",
        "                        accuracy_score(y_test_pred, y_test) * 100, 2\n",
        "                    )  # round to 2 digits xx.yy %\n",
        "\n",
        "                    results[\"supervised_learning_accuracy_easy\"] = supervised_learning_accuracy_easy\n",
        "                    \n",
        "                    dips_xthresh= 0.15\n",
        "                    if 'Pseudo_Labeling' in algorithm_list:\n",
        "\n",
        "                        print(\"Running Pseudo Labeling...\")\n",
        "\n",
        "                        (\n",
        "                            pseudo_labeling_acc_vanilla,\n",
        "                            pseudo_labeling_acc_dips_begin,\n",
        "                            pseudo_labeling_acc_dips_full,\n",
        "                            pseudo_labeling_acc_dips_full2,\n",
        "                            artifacts\n",
        "\n",
        "                            \n",
        "                        ) = run_pseudo(\n",
        "                            x_unlabeled=x_unlabeled,\n",
        "                            x_test=x_test,\n",
        "                            y_test=y_test,\n",
        "                            x_train=x_train,\n",
        "                            y_train=y_train,\n",
        "                            numIters=numIters,\n",
        "                            upper_threshold=upper_threshold,\n",
        "                            nest=nest,\n",
        "                            seed=seed,\n",
        "                            easy_train=easy_train,\n",
        "                            dips_metric=dips_metric,\n",
        "                            dips_xthresh=dips_xthresh,\n",
        "                            dips_ythresh=dips_ythresh,\n",
        "                            verbose=verbose,\n",
        "                            method=method,\n",
        "                            epochs=epochs,\n",
        "                        )\n",
        "\n",
        "                        results[\"pseudo\"] = {\n",
        "                            \"vanilla\": pseudo_labeling_acc_vanilla,\n",
        "                            \"dips_begin\": pseudo_labeling_acc_dips_begin,\n",
        "                            \"dips_full\": pseudo_labeling_acc_dips_full,\n",
        "                            \"dips_full2\": pseudo_labeling_acc_dips_full2,\n",
        "                        }\n",
        "\n",
        "                        # data['pseudo'] = {'vanilla':artifacts['vanilla']['data'], \n",
        "                        #     'dips_begin':artifacts['begin']['data'], \n",
        "                        #     'dips_full':artifacts['full1']['data'], \n",
        "                        #     'dips_full2':artifacts['full2']['data']}\n",
        "                        \n",
        "                        # models['pseudo'] = {'vanilla':artifacts['vanilla']['models'], \n",
        "                        #     'dips_begin':artifacts['begin']['models'], \n",
        "                        #     'dips_full':artifacts['full1']['models'], \n",
        "                        #     'dips_full2':artifacts['full2']['models']}\n",
        "\n",
        "            \n",
        "                    overall_result_dicts.append(results)\n",
        "                #     overall_data_dicts.append(data)\n",
        "                #     overall_model_dicts.append(models)\n",
        "\n",
        "                # overall_result_dicts, overall_data_dicts, overall_model_dicts, datasize\n",
        "\n",
        "                except Exception as e:\n",
        "                    import traceback\n",
        "                    print(traceback.format_exc())\n",
        "                    print(e)\n",
        "                    continue\n",
        "\n",
        "            from src.utils import process_results\n",
        "            results = process_results(results_list=overall_result_dicts,numIters=numIters, end_score=True)\n",
        "\n",
        "            results_store[method] = results\n",
        "            results_store['dataset'] = dataset\n",
        "                        \n",
        "            \n",
        "\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(traceback.format_exc())\n",
        "            print(e)\n",
        "            continue\n",
        "    \n",
        "    # save results_store to pickle\n",
        "    import pickle\n",
        "    filename = f\"../results/lnl_{dataset}.pickle\"\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(results_store, f)\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'filter': {'fully_supervised_learning_accuracy': {'acc_mean': 84.71,\n",
              "   'acc_se': 0.3099999999999993},\n",
              "  'supervised_learning_accuracy': {'acc_mean': 81.51333333333334,\n",
              "   'acc_se': 0.33348329959851386},\n",
              "  'supervised_learning_accuracy_easy': {'acc_mean': 81.04,\n",
              "   'acc_se': 0.3686461718233367},\n",
              "  'pseudo': {'vanilla_mean': 81.92666666666666,\n",
              "   'vanilla_se': 0.32793969635352765,\n",
              "   'dips_begin_mean': 82.24,\n",
              "   'dips_begin_se': 0.2666458325194688,\n",
              "   'dips_full_mean': 81.80333333333333,\n",
              "   'dips_full_se': 0.3888587289892192}},\n",
              " 'dataset': 'seer',\n",
              " 'basicfilter': {'fully_supervised_learning_accuracy': {'acc_mean': 84.71,\n",
              "   'acc_se': 0.3099999999999993},\n",
              "  'supervised_learning_accuracy': {'acc_mean': 81.51333333333334,\n",
              "   'acc_se': 0.33348329959851386},\n",
              "  'supervised_learning_accuracy_easy': {'acc_mean': 77.61,\n",
              "   'acc_se': 1.818635019274987},\n",
              "  'pseudo': {'vanilla_mean': 81.92666666666666,\n",
              "   'vanilla_se': 0.32793969635352765,\n",
              "   'dips_begin_mean': 78.21666666666667,\n",
              "   'dips_begin_se': 2.052821906005919,\n",
              "   'dips_full_mean': 76.25666666666667,\n",
              "   'dips_full_se': 2.3074974419150345}},\n",
              " 'loss': {'fully_supervised_learning_accuracy': {'acc_mean': 84.71,\n",
              "   'acc_se': 0.3099999999999993},\n",
              "  'supervised_learning_accuracy': {'acc_mean': 81.51333333333334,\n",
              "   'acc_se': 0.33348329959851386},\n",
              "  'supervised_learning_accuracy_easy': {'acc_mean': 81.39666666666666,\n",
              "   'acc_se': 0.4693375946776225},\n",
              "  'pseudo': {'vanilla_mean': 81.92666666666666,\n",
              "   'vanilla_se': 0.32793969635352765,\n",
              "   'dips_begin_mean': 81.76,\n",
              "   'dips_begin_se': 0.491663841799798,\n",
              "   'dips_full_mean': 80.42333333333333,\n",
              "   'dips_full_se': 0.7449011865863667}},\n",
              " 'fine': {'fully_supervised_learning_accuracy': {'acc_mean': 84.71,\n",
              "   'acc_se': 0.3099999999999993},\n",
              "  'supervised_learning_accuracy': {'acc_mean': 81.51333333333334,\n",
              "   'acc_se': 0.33348329959851386},\n",
              "  'supervised_learning_accuracy_easy': {'acc_mean': 79.02666666666666,\n",
              "   'acc_se': 0.8719008608271407},\n",
              "  'pseudo': {'vanilla_mean': 81.92666666666666,\n",
              "   'vanilla_se': 0.32793969635352765,\n",
              "   'dips_begin_mean': 79.95,\n",
              "   'dips_begin_se': 0.5853488988059437,\n",
              "   'dips_full_mean': 76.36666666666667,\n",
              "   'dips_full_se': 1.162430404129403}}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM1YDnCNhxzVnGQY13tScTc",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "demo_ConfidentSinkhornAllocation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ssl_project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
