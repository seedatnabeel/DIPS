{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uy43buLIvVyv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from src.dips_selector import *\n",
        "from src.data_loader import *\n",
        "from src.baseline_functions import *\n",
        "from src.data_loader import * \n",
        "\n",
        "import traceback"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "overall_result_dicts = []\n",
        "overall_data_dicts = []\n",
        "overall_model_dicts = []\n",
        "\n",
        "dips_metric = 'aleatoric'\n",
        "dips_ythresh = 0.2\n",
        "\n",
        "algorithm_list=['Fully Supervised', 'Supervised_Learning','Pseudo_Labeling'] # Others: UPS, CSA, FlexMatch, SLA\n",
        "        \n",
        "dataset_name = 'compas'\n",
        "seed=42\n",
        "nest=100\n",
        "prop_lab = 0.1\n",
        "prop_data=1\n",
        "num_XGB_models=5\n",
        "numTrials=1\n",
        "numIters=5\n",
        "upper_threshold=0.8\n",
        "verbose=False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nabeel/Documents/Project code/ssl_dcai/ssl_dcai_env/lib/python3.10/site-packages/openml/datasets/functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
            "  warnings.warn(\n",
            "/Users/nabeel/Documents/Project code/DIPS/src/data_loader.py:320: FutureWarning: Support for `dataset_format='array'` will be removed in 0.15,start using `dataset_format='dataframe' to ensure your code will continue to work. You can use the dataframe's `to_numpy` function to continue using numpy arrays.\n",
            "  X, y, categorical_indicator, attribute_names = dataset.get_data(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/1\n",
            "# total samples = 5278 (1 - prop)\n",
            "# training points = 422\n",
            "# test points = 1056\n",
            "# unlabelled points = 3800\n",
            "Training Supervised model...\n",
            "Running Pseudo Labeling...\n",
            "===== Pseudo_Labeling\n",
            "[230 192]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nabeel/Documents/Project code/ssl_dcai/ssl_dcai_env/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nabeel/Documents/Project code/ssl_dcai/ssl_dcai_env/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Pseudo_Labeling\n",
            "[116  88]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n",
            "===== Pseudo_Labeling\n",
            "[116  88]\n",
            "n iterations 5\n",
            "iteration  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nabeel/Documents/Project code/ssl_dcai/ssl_dcai_env/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nabeel/Documents/Project code/ssl_dcai/ssl_dcai_env/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Pseudo_Labeling\n",
            "[230 192]\n",
            "n iterations 5\n",
            "iteration  0\n",
            "iteration  1\n",
            "iteration  2\n",
            "iteration  3\n",
            "iteration  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    for i in tqdm(range(numTrials)):\n",
        "    \n",
        "        seed+=1\n",
        "        print(f\"Trial {i+1}/{numTrials}\")\n",
        "        results = {}\n",
        "        data = {}\n",
        "        models = {}\n",
        "\n",
        "        df_feat, df_label, df = get_data(dataset=dataset_name, prop=prop_data)\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(\n",
        "            df_feat, df_label, test_size=0.2, random_state=seed\n",
        "        )\n",
        "\n",
        "        x_train, x_unlabeled, y_train, y_unlabeled = train_test_split(\n",
        "            x_train, y_train, train_size=prop_lab, random_state=seed\n",
        "        )\n",
        "\n",
        "        x_unlabeled, x_test, y_test, x_train, y_train = (\n",
        "            np.asarray(x_unlabeled),\n",
        "            np.asarray(x_test),\n",
        "            np.asarray(y_test),\n",
        "            np.asarray(x_train),\n",
        "            np.asarray(y_train),\n",
        "        )\n",
        "            \n",
        "\n",
        "        datasize = x_train.shape\n",
        "\n",
        "        total_samples = len(x_train) + len(x_test) + len(x_unlabeled)\n",
        "\n",
        "        print(f\"# total samples = {total_samples} ({prop_data} - prop)\")\n",
        "\n",
        "        print(f\"# training points = {y_train.shape[0]}\")\n",
        "\n",
        "        print(f\"# test points = {y_test.shape[0]}\")\n",
        "\n",
        "        print(f\"# unlabelled points = {x_unlabeled.shape[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "        # # Supervised learning - Train an XGBoost model\n",
        "        param = {}\n",
        "        param[\"booster\"] = \"gbtree\"\n",
        "        param[\"objective\"] = \"binary:logistic\"\n",
        "        param[\"verbosity\"] = 0\n",
        "        param[\"n_estimators\"] = nest\n",
        "        param[\"silent\"] = 1\n",
        "        param[\"seed\"] = seed\n",
        "\n",
        "\n",
        "        print(\"Training Supervised model...\")\n",
        "        # create XGBoost instance with default hyper-parameters\n",
        "        xgb = XGBClassifier(**param)\n",
        "\n",
        "        xgb.fit(x_train, y_train)\n",
        "\n",
        "        # evaluate the performance on the test set\n",
        "        y_test_pred = xgb.predict(x_test)\n",
        "        supervised_learning_accuracy = np.round(\n",
        "            accuracy_score(y_test_pred, y_test) * 100, 2\n",
        "        )  # round to 2 digits xx.yy %\n",
        "\n",
        "        results[\"supervised_learning_accuracy\"] = supervised_learning_accuracy\n",
        "\n",
        "        # Run dips\n",
        "        dips_xgb = DIPS_selector(X=x_train, y=y_train)\n",
        "\n",
        "        for i in range(1, nest):\n",
        "            # *** Characterize with dips [LINE 2] ***\n",
        "            dips_xgb.on_epoch_end(clf=xgb, iteration=i)\n",
        "\n",
        "        # *** Access metrics ***\n",
        "        if dips_metric == \"aleatoric\":\n",
        "            dips_xmetric = dips_xgb.aleatoric\n",
        "        elif dips_metric == \"epistemic\":\n",
        "            dips_xmetric = dips_xgb.variability\n",
        "        elif dips_metric == \"entropy\":\n",
        "            dips_xmetric = dips_xgb.entropy\n",
        "        elif dips_metric == \"mi\":\n",
        "            dips_xmetric = dips_xgb.mi\n",
        "\n",
        "        confidence = dips_xgb.confidence\n",
        "\n",
        "        assert len(confidence) == len(y_train)\n",
        "\n",
        "        # adaptive threshold\n",
        "        dips_xthresh =  0.75*(np.max(dips_xmetric)-np.min(dips_xmetric))\n",
        "\n",
        "        easy_train, ambig_train, hard_train = get_groups(\n",
        "            confidence=confidence,\n",
        "            aleatoric_uncertainty=dips_xmetric,\n",
        "            dips_xthresh=dips_xthresh,\n",
        "            dips_ythresh=dips_ythresh,\n",
        "        )\n",
        "\n",
        "        if 'Pseudo_Labeling' in algorithm_list:\n",
        "\n",
        "            print(\"Running Pseudo Labeling...\")\n",
        "\n",
        "            (\n",
        "                pseudo_labeling_acc_vanilla,\n",
        "                pseudo_labeling_acc_dips_begin,\n",
        "                pseudo_labeling_acc_dips_full,\n",
        "                pseudo_labeling_acc_dips_partial,\n",
        "                artifacts\n",
        "\n",
        "                \n",
        "            ) = run_pseudo(\n",
        "                x_unlabeled=x_unlabeled,\n",
        "                x_test=x_test,\n",
        "                y_test=y_test,\n",
        "                x_train=x_train,\n",
        "                y_train=y_train,\n",
        "                numIters=numIters,\n",
        "                upper_threshold=upper_threshold,\n",
        "                nest=nest,\n",
        "                seed=seed,\n",
        "                easy_train=easy_train,\n",
        "                dips_metric=dips_metric,\n",
        "                dips_xthresh=dips_xthresh,\n",
        "                dips_ythresh=dips_ythresh,\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            results[\"pseudo\"] = {\n",
        "                \"vanilla\": pseudo_labeling_acc_vanilla,\n",
        "                \"dips_full\": pseudo_labeling_acc_dips_full,\n",
        "            }\n",
        "\n",
        "            data['pseudo'] = {'vanilla':artifacts['vanilla']['data'], \n",
        "                'dips_full':artifacts['full']['data'], \n",
        "                }\n",
        "            \n",
        "            models['pseudo'] = {'vanilla':artifacts['vanilla']['models'], \n",
        "                'dips_full':artifacts['full']['models'], \n",
        "                }\n",
        "\n",
        "        if 'CSA' in algorithm_list:\n",
        "            print(\"Running CSA...\")\n",
        "            (\n",
        "                csa_acc_vanilla,\n",
        "                csa_acc_dips_begin,\n",
        "                csa_acc_dips_full,\n",
        "                csa_acc_dips_partial,\n",
        "                artifacts\n",
        "\n",
        "            ) = run_CSA(\n",
        "                x_unlabeled=x_unlabeled,\n",
        "                x_test=x_test,\n",
        "                y_test=y_test,\n",
        "                x_train=x_train,\n",
        "                y_train=y_train,\n",
        "                numIters=numIters,\n",
        "                num_XGB_models=num_XGB_models,\n",
        "                nest=nest,\n",
        "                seed=seed,\n",
        "                easy_train=easy_train,\n",
        "                dips_metric=dips_metric,\n",
        "                dips_xthresh=dips_xthresh,\n",
        "                dips_ythresh=dips_ythresh,\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            results[\"csa\"] = {\n",
        "                \"vanilla\": csa_acc_vanilla,\n",
        "                \"dips_full\": csa_acc_dips_full,\n",
        "            }\n",
        "\n",
        "            data['csa'] = {'vanilla':artifacts['vanilla']['data'], \n",
        "                'dips_full':artifacts['full']['data'], \n",
        "                }\n",
        "            \n",
        "            models['csa'] = {'vanilla':artifacts['vanilla']['models'], \n",
        "                'dips_full':artifacts['full']['models'], \n",
        "                }\n",
        "\n",
        "\n",
        "        if 'SLA' in algorithm_list:\n",
        "            print(\"Running SLA...\")\n",
        "            (\n",
        "                sla_acc_vanilla,\n",
        "                sla_acc_dips_begin,\n",
        "                sla_acc_dips_full,\n",
        "                sla_acc_dips_partial,\n",
        "                artifacts\n",
        "\n",
        "            ) = run_SLA(\n",
        "                x_unlabeled=x_unlabeled,\n",
        "                x_test=x_test,\n",
        "                y_test=y_test,\n",
        "                x_train=x_train,\n",
        "                y_train=y_train,\n",
        "                numIters=numIters,\n",
        "                num_XGB_models=num_XGB_models,\n",
        "                nest=nest,\n",
        "                seed=seed,\n",
        "                easy_train=easy_train,\n",
        "                dips_metric=dips_metric,\n",
        "                dips_xthresh=dips_xthresh,\n",
        "                dips_ythresh=dips_ythresh,\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            results[\"sla\"] = {\n",
        "                \"vanilla\": sla_acc_vanilla,\n",
        "                \"dips_full\": sla_acc_dips_full,\n",
        "    \n",
        "            }\n",
        "\n",
        "            data['sla'] = {'vanilla':artifacts['vanilla']['data'], \n",
        "                'dips_full':artifacts['full']['data'], \n",
        "                }\n",
        "            models['sla'] = {'vanilla':artifacts['vanilla']['models'], \n",
        "                'dips_full':artifacts['full']['models'], \n",
        "\n",
        "                }\n",
        "\n",
        "\n",
        "\n",
        "        if 'UPS' in algorithm_list:\n",
        "            print(\"Running UPS...\")\n",
        "            (\n",
        "                ups_acc_vanilla,\n",
        "                ups_acc_dips_begin,\n",
        "                ups_acc_dips_full,\n",
        "                ups_acc_dips_partial,\n",
        "                artifacts\n",
        "            ) = run_UPS(\n",
        "                x_unlabeled=x_unlabeled,\n",
        "                x_test=x_test,\n",
        "                y_test=y_test,\n",
        "                x_train=x_train,\n",
        "                y_train=y_train,\n",
        "                numIters=numIters,\n",
        "                num_XGB_models=num_XGB_models,\n",
        "                nest=nest,\n",
        "                seed=seed,\n",
        "                easy_train=easy_train,\n",
        "                dips_metric=dips_metric,\n",
        "                dips_xthresh=dips_xthresh,\n",
        "                dips_ythresh=dips_ythresh,\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            results[\"ups\"] = {\n",
        "                \"vanilla\": ups_acc_vanilla,\n",
        "                \"dips_full\": ups_acc_dips_full,\n",
        "            }\n",
        "\n",
        "            data['ups'] = {'vanilla':artifacts['vanilla']['data'], \n",
        "                'dips_full':artifacts['full']['data'], \n",
        "                }\n",
        "            \n",
        "            models['ups'] = {'vanilla':artifacts['vanilla']['models'], \n",
        "                'dips_full':artifacts['full']['models'], \n",
        "                }\n",
        "\n",
        "\n",
        "        if 'FlexMatch' in algorithm_list:\n",
        "            print(\"Running Flex match...\")\n",
        "            (\n",
        "                flex_acc_vanilla,\n",
        "                flex_acc_dips_begin,\n",
        "                flex_acc_dips_full,\n",
        "                flex_acc_dips_partial,\n",
        "                artifacts\n",
        "            ) = run_FlexMatch(\n",
        "                x_unlabeled=x_unlabeled,\n",
        "                x_test=x_test,\n",
        "                y_test=y_test,\n",
        "                x_train=x_train,\n",
        "                y_train=y_train,\n",
        "                upper_threshold=upper_threshold,\n",
        "                numIters=numIters,\n",
        "                nest=nest,\n",
        "                seed=seed,\n",
        "                easy_train=easy_train,\n",
        "                dips_metric=dips_metric,\n",
        "                dips_xthresh=dips_xthresh,\n",
        "                dips_ythresh=dips_ythresh,\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            results[\"flex\"] = {\n",
        "                \"vanilla\": flex_acc_vanilla,\n",
        "                \"dips_full\": flex_acc_dips_full,\n",
        "            }\n",
        "\n",
        "            data['flex'] = {'vanilla':artifacts['vanilla']['data'], \n",
        "                'dips_full':artifacts['full']['data'], \n",
        "                }\n",
        "            \n",
        "            models['flex'] = {'vanilla':artifacts['vanilla']['models'],  \n",
        "                'dips_full':artifacts['full']['models'], \n",
        "                }\n",
        "\n",
        "        overall_result_dicts.append(results)\n",
        "        overall_data_dicts.append(data)\n",
        "        overall_model_dicts.append(models)\n",
        "\n",
        "    overall_result_dicts, overall_data_dicts, overall_model_dicts, datasize\n",
        "\n",
        "            \n",
        "except:\n",
        "    print(traceback.format_exc())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'supervised_learning_accuracy': 60.7,\n",
              "  'pseudo': {'vanilla': [60.7, 60.98, 62.31, 61.36, 61.84],\n",
              "   'dips_full': [64.49, 66.0, 64.96, 65.25, 64.87]}}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overall_result_dicts"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM1YDnCNhxzVnGQY13tScTc",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "demo_ConfidentSinkhornAllocation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ssl_project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
